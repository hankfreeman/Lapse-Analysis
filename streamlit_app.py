import streamlit as st
import pandas as pd
import numpy as np
import warnings
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import glob
import os
from typing import Optional, List, Dict, Any, Tuple

# Import for the custom color scale styling
from matplotlib.colors import LinearSegmentedColormap, to_hex

# --- 1. CONFIGURATION & FILE PATHS (SET THIS UP FIRST!) ---
# âš ï¸ IMPORTANT: Update this path to your actual FTP folder location âš ï¸
BASE_PATH = 'C:/Users/HFreeman/Desktop/FTP/'

POLICY_FILE_PATTERN = os.path.join(BASE_PATH, 'Policies_DBGA_*.csv')
NSF_FILE_PATTERN = os.path.join(BASE_PATH, 'NSF_Activity_DBGA_*.csv')
CANCELLED_FILE_PATTERN = os.path.join(BASE_PATH, 'Cancelled_*.csv')

ANALYSIS_END_DATE = pd.to_datetime('2025-11-07')

# Suppress the SettingWithCopyWarning
warnings.filterwarnings('ignore', message='A value is trying to be set on a copy of a slice from a DataFrame.')

# Underwriting Class Mapping
uw_class_mapping: Dict[str, str] = {
Â  Â  'W12GI': 'GI', 'W12GI FL': 'GI', 'W12GI KS': 'GI', 'W12GI KY': 'GI', 'W12GI MO': 'GI',
Â  Â  'W12GI MT': 'GI', 'W12GI PA': 'GI', 'W12GI SC': 'GI', 'W12GI TX': 'GI', 'W12GI WI': 'GI',
Â  Â  'W12GI WY': 'GI', 'WL12G': 'Graded', 'WL12G FL': 'Graded', 'WL12G KS': 'Graded',
Â  Â  'WL12G KY': 'Graded', 'WL12G MO': 'Graded', 'WL12G MT': 'Graded', 'WL12G PA': 'Graded',
Â  Â  'WL12G SC': 'Graded', 'WL12G TX': 'Graded', 'WL12G WI': 'Graded', 'WL12G WY': 'Graded',
Â  Â  'WL12P': 'Preferred', 'WL12P FL': 'Preferred', 'WL12P KS': 'Preferred', 'WL12P KY': 'Preferred',
Â  Â  'WL12P MO': 'Preferred', 'WL12P MT': 'Preferred', 'WL12P PA': 'Preferred', 'WL12P SC': 'Preferred',
Â  Â  'WL12P TX': 'Preferred', 'WL12P WI': 'Preferred', 'WL12P WY': 'Preferred', 'WL12S': 'Standard',
Â  Â  'WL12S FL': 'Standard', 'WL12S KS': 'Standard', 'WL12S KY': 'Standard', 'WL12S MO': 'Standard',
Â  Â  'WL12S MT': 'Standard', 'WL12S PA': 'Standard', 'WL12S SC': 'Standard', 'WL12S TX': 'Standard',
Â  Â  'WL12S WI': 'Standard', 'WL12S WY': 'Standard'
}
MAPPING_PLAN_CODES = list(uw_class_mapping.keys())

# Reason Definitions
# REASON_MAP is kept as it's used in assign_termination_reason which helps determine the 'Lapsed' status detail
REASON_MAP = {
Â  Â  r'(?i)Check Returned': 'Check Returned',
Â  Â  r'(?i)NSF \(OTHER|NSF': 'NSF',
Â  Â  r'(?i)Account Closed': 'Account Closed',
Â  Â  r'(?i)Credit Card Declined': 'Credit Card Declined',
Â  Â  r'(?i)Stop Payment': 'Stop Payment'
}

# Type hint for the complex return tuple (reasons_df is removed)
AnalysisResult = Tuple[pd.DataFrame, int, pd.Series, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]

# --- 2. LOCAL FILE HANDLERS AND MERGE ---

def get_most_recent_file(pattern: str) -> Optional[str]:
Â  Â  """ Finds the most recently modified file matching a pattern. """
Â  Â  files = glob.glob(pattern)
Â  Â  if not files:
Â  Â  Â  Â  return None
Â  Â  files.sort(key=os.path.getmtime, reverse=True)
Â  Â  return files[0]

def clean_policy_nbr(df: pd.DataFrame, col_name: str) -> pd.DataFrame:
Â  Â  """ Standardizes policy_nbr column to 'policy_nbr' and cleans data. """
Â  Â  if col_name in df.columns:
Â  Â  Â  Â  df['policy_nbr'] = df[col_name].astype(str).str.strip()
Â  Â  Â  Â  if col_name != 'policy_nbr':
Â  Â  Â  Â  Â  Â  df = df.drop(columns=[col_name], errors='ignore')
Â  Â  return df

def map_nsf_reason(df: pd.DataFrame) -> pd.DataFrame:
Â  Â  """ Maps detailed NSF reasons to the simplified list. """
Â  Â  df['Final_Reason'] = 'NSF: Unknown'
Â  Â  for pattern, reason in REASON_MAP.items():
Â  Â  Â  Â  mask = df['Clean_Reason'].astype(str).str.contains(pattern, case=False, na=False)
Â  Â  Â  Â  df.loc[mask, 'Final_Reason'] = reason
Â  Â  return df

@st.cache_data
def load_and_merge_data(
Â  Â  policy_pattern: str,Â 
Â  Â  nsf_pattern: str,Â 
Â  Â  cancelled_pattern: str,
Â  Â  cache_buster: Optional[float] = None
) -> Tuple[pd.DataFrame, pd.Timestamp]: # Modified return type to include earliest_date
Â  Â  """Â 
Â  Â  Loads all files from the local directory, cleans policy numbers, merges, and determines the final reason.
Â  Â  FIXES:
Â  Â  - Explicitly converts date columns to string before parsing.
Â  Â  - Explicitly parses 'issue_date' and 'term_date' (now 'term_date', 'issue_date') as YYYYMMDD.
Â  Â  """
Â  Â Â 
Â  Â  # Implementation change: always load the bundled `randompolicydata.csv` in the repo directory
Â  Â  csv_path = os.path.join(os.path.dirname(__file__), 'randompolicydata.csv')

Â  Â  if not os.path.exists(csv_path):
Â  Â  Â  Â  # Return empty with NaT so callers can show the existing error message
Â  Â  Â  Â  return pd.DataFrame(), pd.NaT

Â  Â  # Read the bundled CSV
Â  Â  df_policies = pd.read_csv(csv_path, encoding='unicode_escape', sep=',', on_bad_lines='skip', engine='python')
Â  Â Â 
Â  Â  # Normalize column names - This handles the 'Issue Date' -> 'issue_date' and 'Term Date' -> 'term_date' transformation.
Â  Â  df_policies.columns = df_policies.columns.str.strip().str.replace(r'[^\w\s]', '', regex=True).str.lower().str.replace(r'\s+', '_', regex=True)
Â  Â Â 
Â  Â  df_policies = clean_policy_nbr(df_policies, col_name='policy_nbr')

Â  Â  context_cols_to_keep = ['policy_nbr', 'term_date', 'issue_date', 'plan_code', 'tobacco', 'gender', 'app_recvd_date']
Â  Â  df_policies = df_policies[[col for col in context_cols_to_keep if col in df_policies.columns]]

Â  Â  # --- FIX 1 & 4: Robust Date Parsing ---
Â  Â  date_cols_to_parse = ['app_recvd_date', 'issue_date', 'term_date']
Â  Â  earliest_date = pd.NaT
Â  Â Â 
Â  Â  for col in date_cols_to_parse:
Â  Â  Â  Â  if col in df_policies.columns:
Â  Â  Â  Â  Â  Â  # FIX: Ensure dates are strings before parsing (handles numeric YYYYMMDD being read as int/float)
Â  Â  Â  Â  Â  Â  df_policies[col] = df_policies[col].astype(str).str.strip().str.replace(r'\.0$', '', regex=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # FIX: Use the correct format for YYYYMMDD
Â  Â  Â  Â  Â  Â  df_policies[col] = pd.to_datetime(df_policies[col], format='%Y%m%d', errors='coerce')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Debugging check:
Â  Â  Â  Â  Â  Â  # st.write(f"Parsed {col}: {df_policies[col].notna().sum()} valid dates")


Â  Â  # Compute earliest date
Â  Â  if 'app_recvd_date' in df_policies.columns:
Â  Â  Â  Â  earliest_date = df_policies['app_recvd_date'].min()
Â  Â Â 
Â  Â  # --- End Date Parsing Fixes ---

Â  Â  if 'plan_code' in df_policies.columns:
Â  Â  Â  Â  df_policies['plan_code'] = df_policies['plan_code'].astype(str).str.strip().str.upper()

Â  Â  # There is no NSF/Cancelled merging when using the bundled CSV; set a default clean reason
Â  Â  df_policies['clean_reason'] = 'Unknown'

Â  Â  # DEBUGGING: Print data quality stats
Â  Â  # st.write(f"Total records after load: {len(df_policies):,}")
Â  Â  # st.write(f"Valid App Recvd Date: {df_policies['app_recvd_date'].notna().sum():,}")
Â  Â  # st.write(f"Valid Issue Date: {df_policies['issue_date'].notna().sum():,}")
Â  Â  # st.write(f"Valid Term Date: {df_policies['term_date'].notna().sum():,}")

Â  Â  return df_policies, earliest_date


def assign_termination_reason(df: pd.DataFrame) -> pd.DataFrame:
Â  Â  """ Assigns the final Termination_Reason based on the merged 'clean_reason' and Lapsed status. """
Â  Â  # Keep this function as 'Termination_Reason' is used to identify terminated policies
Â  Â  # even if the detailed analysis is removed.
Â  Â  df['Termination_Reason'] = df['clean_reason'].where(
Â  Â  Â  Â  df['Lapsed'] == 1,Â 
Â  Â  Â  Â  'Active/Not Lapsed'
Â  Â  )
Â  Â  for col in ['Termination_Reason', 'clean_reason']:
Â  Â  Â  Â  if col in df.columns:
Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(str).str.strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  for pattern, reason in REASON_MAP.items():
Â  Â  Â  Â  mask = df['Termination_Reason'].astype(str).str.contains(pattern, case=False, na=False)
Â  Â  Â  Â  df.loc[mask, 'Termination_Reason'] = reason
Â  Â  Â  Â  Â  Â Â 
Â  Â  return df

def calculate_initial_mixes(df_cohort: pd.DataFrame) -> pd.DataFrame:
Â  Â  """ Calculates the initial policy mix (proportions) for each cohort. """
Â  Â  mix_data = []
Â  Â Â 
Â  Â  if df_cohort.empty:
Â  Â  Â  Â  return pd.DataFrame()
Â  Â Â 
Â  Â  df_cohort['Initial_Mix_Group'] = 1 # Temporary column for grouping
Â  Â Â 
Â  Â  for segment_col in ['underwriting_class', 'gender', 'tobacco']:
Â  Â  Â  Â  if segment_col in df_cohort.columns:
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Count policies by Cohort and Segment
Â  Â  Â  Â  Â  Â  counts = df_cohort.groupby(['Cohort_Week_Index', segment_col])['policy_nbr'].nunique().reset_index(name='Policy_Count')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Calculate total policies in each Cohort
Â  Â  Â  Â  Â  Â  totals = df_cohort.groupby('Cohort_Week_Index')['policy_nbr'].nunique().reset_index(name='Total_Cohort_Policies')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Merge to calculate proportion
Â  Â  Â  Â  Â  Â  counts = pd.merge(counts, totals, on='Cohort_Week_Index')
Â  Â  Â  Â  Â  Â  counts['Segment'] = segment_col.replace('_', ' ').title()
Â  Â  Â  Â  Â  Â  counts['Proportion'] = (counts['Policy_Count'] / counts['Total_Cohort_Policies']) * 100
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  counts.rename(columns={segment_col: 'Category'}, inplace=True)
Â  Â  Â  Â  Â  Â  mix_data.append(counts)
Â  Â  Â  Â  Â  Â Â 
Â  Â  if mix_data:
Â  Â  Â  Â  return pd.concat(mix_data, ignore_index=True)
Â  Â  return pd.DataFrame()

def calculate_average_months_held(df: pd.DataFrame, analysis_end_date: pd.Timestamp, cohort_dates_df: pd.DataFrame) -> pd.DataFrame:
Â  Â  """
Â  Â  Calculates the actual months a policy was held until termination or the analysis end date.
Â  Â  Returns a DataFrame with segmented averages for each cohort.
Â  Â  """
Â  Â  df_calc = df.copy()

Â  Â  # Calculate actual held duration in days
Â  Â  df_calc['Held_End_Date'] = df_calc['term_date'].where(df_calc['Lapsed'] == 1, analysis_end_date)
Â  Â  # FIX: Ensure 'issue_date' is a datetime type before subtraction
Â  Â  if 'issue_date' in df_calc.columns and pd.api.types.is_datetime64_any_dtype(df_calc['issue_date']):
Â  Â  Â  Â  df_calc['Held_Duration_Days'] = (df_calc['Held_End_Date'] - df_calc['issue_date']).dt.days
Â  Â  else:
Â  Â  Â  Â  # Fallback if issue_date is still problematic
Â  Â  Â  Â  df_calc['Held_Duration_Days'] = 0Â 
Â  Â  Â  Â Â 
Â  Â  # Convert days to months
Â  Â  df_calc['Months_Held'] = (df_calc['Held_Duration_Days'] / 30.4375).clip(lower=0)

Â  Â  # Define all grouping columns and filter for existence
Â  Â  grouping_cols = ['Cohort_Week_Index', 'underwriting_class', 'gender', 'tobacco']
Â  Â  available_cols = [col for col in grouping_cols if col in df_calc.columns]

Â  Â  segment_data = []
Â  Â Â 
Â  Â  # Calculate Averages by Cohort and Segment
Â  Â  for cohort_index in df_calc['Cohort_Week_Index'].unique():
Â  Â  Â  Â  df_cohort = df_calc[df_calc['Cohort_Week_Index'] == cohort_index]
Â  Â  Â  Â  # Use .get() with a default in case the index is somehow missing (safer)
Â  Â  Â  Â  cohort_start_date = cohort_dates_df.loc[cohort_index, 'Cohort_Start_Date'] if cohort_index in cohort_dates_df.index else pd.NaT
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. Cohort Overall Average
Â  Â  Â  Â  overall_avg = df_cohort['Months_Held'].mean()
Â  Â  Â  Â  segment_data.append({
Â  Â  Â  Â  Â  Â  'Cohort_Index': cohort_index,
Â  Â  Â  Â  Â  Â  'Cohort_Date': cohort_start_date,
Â  Â  Â  Â  Â  Â  'Segment': 'Cohort Overall',Â 
Â  Â  Â  Â  Â  Â  'Category': 'Total',Â 
Â  Â  Â  Â  Â  Â  'Avg_Months_Held': overall_avg,Â 
Â  Â  Â  Â  Â  Â  'Policies': len(df_cohort)
Â  Â  Â  Â  })

Â  Â  Â  Â  # 2. Segment Averages within the Cohort
Â  Â  Â  Â  for col in available_cols:
Â  Â  Â  Â  Â  Â  if col != 'Cohort_Week_Index':
Â  Â  Â  Â  Â  Â  Â  Â  segment_avg = df_cohort.groupby(col)['Months_Held'].agg(['mean', 'count']).reset_index()
Â  Â  Â  Â  Â  Â  Â  Â  segment_avg.columns = ['Category', 'Avg_Months_Held', 'Policies']
Â  Â  Â  Â  Â  Â  Â  Â  segment_avg['Segment'] = col.replace('_', ' ').title()
Â  Â  Â  Â  Â  Â  Â  Â  segment_avg['Cohort_Index'] = cohort_index
Â  Â  Â  Â  Â  Â  Â  Â  segment_avg['Cohort_Date'] = cohort_start_date
Â  Â  Â  Â  Â  Â  Â  Â  segment_data.extend(segment_avg.to_dict('records'))

Â  Â  return pd.DataFrame(segment_data)


@st.cache_data
def perform_cohort_analysis(
Â  Â  df: pd.DataFrame,Â 
Â  Â  cohort_start_date: pd.Timestamp,
Â  Â  # The filter now accepts a generic list of selected plan codes
Â  Â  selected_plans: list[str],
Â  Â  selected_tobacco: list[str],
Â  Â  selected_gender: list[str],
Â  Â  selected_case_status: list[str]Â 
) -> AnalysisResult: # MODIFIED: Removed pd.DataFrame for reasons_df
Â  Â  """Â 
Â  Â  Performs the full weekly cohort lapse analysis, now including Average Months Held and Mix calculation.
Â  Â  FIX: Removed redundant date parsing for issue_date/term_date since it's now handled robustly in load_and_merge_data.
Â  Â  """
Â  Â Â 
Â  Â  df_filtered = df.copy()

Â  Â  # --- Data Cleaning and Filtering ---
Â  Â  if 'plan_code' not in df_filtered.columns or 'policy_nbr' not in df_filtered.columns:
Â  Â  Â  Â  # Return the extended tuple with empty dataframes
Â  Â  Â  Â  return pd.DataFrame(), 0, pd.Series(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame() # MODIFIED: Removed empty DF for reasons_df

Â  Â  # Apply the plan code filter immediately
Â  Â  if selected_plans:
Â  Â  Â  Â  df_filtered = df_filtered[df_filtered['plan_code'].isin(selected_plans)].copy()
Â  Â  Â  Â Â 
Â  Â  # Map to underwriting class *after* filtering (unmapped codes will result in NaN, which is fine)
Â  Â  # Mapping happens here because we need it for segmentation charts later
Â  Â  df_filtered['underwriting_class'] = df_filtered['plan_code'].map(uw_class_mapping)

Â  Â  # Date check: App_recvd_date should be a datetime from load_and_merge_data
Â  Â  if not pd.api.types.is_datetime64_any_dtype(df_filtered['app_recvd_date']):
Â  Â  Â  Â  # Fallback to re-parse if needed, though load_and_merge_data should handle it.
Â  Â  Â  Â  df_filtered['app_recvd_date'] = pd.to_datetime(df_filtered['app_recvd_date'], errors='coerce')


Â  Â  # --- Existing Filters (Plan code filter moved up) ---
Â  Â  if selected_tobacco and 'tobacco' in df_filtered.columns:
Â  Â  Â  Â  df_filtered = df_filtered[df_filtered['tobacco'].isin(selected_tobacco)]
Â  Â  if selected_gender and 'gender' in df_filtered.columns:
Â  Â  Â  Â  df_filtered = df_filtered[df_filtered['gender'].isin(selected_gender)]

Â  Â  df_cohort = df_filtered[df_filtered['app_recvd_date'] >= cohort_start_date].copy()
Â  Â Â 
Â  Â  if df_cohort.empty:
Â  Â  Â  Â  # Return the extended tuple with empty dataframes
Â  Â  Â  Â  return pd.DataFrame(), 0, pd.Series(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame() # MODIFIED: Removed empty DF for reasons_df

Â  Â  df_cohort['Cohort_Week_Index'] = ((df_cohort['app_recvd_date'] - cohort_start_date).dt.days // 7 + 1)
Â  Â  cohort_dates = df_cohort.groupby('Cohort_Week_Index')['app_recvd_date'].min().rename('Cohort_Start_Date')

Â  Â  # --- Lapse Status and Tenure Calculation ---
Â  Â  # FIX 3: Robust Lapsed Policy Check
Â  Â  # A policy is Lapsed (1) if term_date is NOT NaT AND term_date is before or equal to the ANALYSIS_END_DATE
Â  Â  df_cohort['Lapsed'] = (df_cohort['term_date'].notna() & (df_cohort['term_date'] <= ANALYSIS_END_DATE)).astype(int)
Â  Â Â 
Â  Â  # DEBUGGING: Check lapsed policy count
Â  Â  # st.write(f"Total policies in analysis: {len(df_cohort):,}")
Â  Â  # st.write(f"Lapsed policies: {df_cohort['Lapsed'].sum():,}")
Â  Â Â 
Â  Â  # --- Apply the Case Status Filter ---
Â  Â  df_cohort['Case_Status'] = df_cohort['Lapsed'].apply(lambda x: 'Inactive' if x == 1 else 'Active')
Â  Â Â 
Â  Â  if selected_case_status:
Â  Â  Â  Â  df_cohort = df_cohort[df_cohort['Case_Status'].isin(selected_case_status)].copy()
Â  Â  Â  Â Â 
Â  Â  if df_cohort.empty:
Â  Â  Â  Â  # After filtering by status, we might have an empty DataFrame again
Â  Â  Â  Â  return pd.DataFrame(), 0, pd.Series(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame() # MODIFIED: Removed empty DF for reasons_df

Â  Â  # Recalculate Lapsed status based on the *filtered* set (important for charts)
Â  Â  df_cohort['Lapsed'] = (df_cohort['term_date'].notna() & (df_cohort['term_date'] <= ANALYSIS_END_DATE)).astype(int)
Â  Â Â 
Â  Â  df_cohort['Actual_Lapse_Date'] = df_cohort['term_date'].where(df_cohort['Lapsed'] == 1, pd.NaT)
Â  Â Â 
Â  Â  # Calculate Policy Month for cross-cohort analysis (REASON ANALYSIS IS REMOVED, BUT KEEPING Policy_Month FOR REFERENCE IF NEEDED)
Â  Â  # FIX: Ensure issue_date is valid before calculation
Â  Â  if 'issue_date' in df_cohort.columns and pd.api.types.is_datetime64_any_dtype(df_cohort['issue_date']):
Â  Â  Â  Â  df_cohort['Policy_Month'] = (((df_cohort['Actual_Lapse_Date'] - df_cohort['issue_date']).dt.days / 30.4375).fillna(0).astype(int) + 1).clip(upper=100)
Â  Â  else:
Â  Â  Â  Â  df_cohort['Policy_Month'] = 0
Â  Â  Â  Â Â 
Â  Â  df_cohort['Effective_End_Date'] = df_cohort['Actual_Lapse_Date'].fillna(ANALYSIS_END_DATE)
Â  Â  df_cohort['Tenure_Weeks'] = ((df_cohort['Effective_End_Date'] - df_cohort['app_recvd_date']).dt.days / 7)
Â  Â  df_cohort['Lapse_Week'] = ((df_cohort['Actual_Lapse_Date'] - df_cohort['app_recvd_date']).dt.days // 7 + 1)
Â  Â  max_overall_tenure_week = int(df_cohort['Tenure_Weeks'].max()) + 1

Â  Â  # --- ASSIGN TERMINATION REASONS ---
Â  Â  df_cohort = assign_termination_reason(df_cohort) # Keep this for the Policy Detail Table if the user wants it.

Â  Â  # --- Cohort Lapse Matrix Calculation (for Retention Plot) ---
Â  Â  cohort_sizes = df_cohort.groupby('Cohort_Week_Index')['policy_nbr'].nunique().rename('Total_Policies')
Â  Â  total_lapsed_policies = df_cohort[df_cohort['Lapsed'] == 1]['policy_nbr'].nunique()

Â  Â  # DEBUGGING: Check total lapsed policies
Â  Â  # st.write(f"Total lapsed policies after filters: {total_lapsed_policies:,}")

Â  Â  lapse_counts = (df_cohort[df_cohort['Lapsed'] == 1].groupby(['Cohort_Week_Index', 'Lapse_Week'])['policy_nbr'].nunique().rename('Lapses'))
Â  Â  lapse_matrix = lapse_counts.reset_index().pivot_table(index='Cohort_Week_Index', columns='Lapse_Week', values='Lapses', fill_value=0)
Â  Â Â 
Â  Â  all_lapse_weeks = pd.Index(range(1, max_overall_tenure_week + 1), name='Lapse_Week')
Â  Â  lapse_matrix = lapse_matrix.reindex(columns=all_lapse_weeks, fill_value=0)
Â  Â  cumulative_lapses = lapse_matrix.cumsum(axis=1)
Â  Â Â 
Â  Â  # Handle division by zero if a cohort has zero policies after filtering
Â  Â  cumulative_lapse_rate = cumulative_lapses.div(cohort_sizes, axis=0) * 100
Â  Â  retention_rate = 100 - cumulative_lapse_rateÂ 
Â  Â Â 
Â  Â  # --- Dynamic Masking ---
Â  Â  retention_rate.columns = [f'Week_{int(w)}' for w in retention_rate.columns]
Â  Â  max_cohort_tenure = df_cohort.groupby('Cohort_Week_Index')['Tenure_Weeks'].max()
Â  Â  mask = pd.DataFrame(False, index=retention_rate.index, columns=retention_rate.columns)
Â  Â  week_indices = np.array([int(col.split('_')[1]) for col in retention_rate.columns])
Â  Â  for cohort_index in retention_rate.index:
Â  Â  Â  Â  tenure_threshold = max_cohort_tenure.loc[cohort_index]
Â  Â  Â  Â  mask.loc[cohort_index] = week_indices > np.floor(tenure_threshold)
Â  Â  retention_rate[mask] = np.nan
Â  Â  retention_rate.index.name = 'Cohort_Week_Index'
Â  Â Â 
Â  Â  # --- Data Preparation for Plots/Tables & Persistency ---
Â  Â  mix_df = df_cohort[['Cohort_Week_Index', 'Tenure_Weeks', 'Lapsed', 'Lapse_Week', 'underwriting_class', 'gender', 'tobacco', 'issue_date', 'term_date']].copy()
Â  Â Â 
Â  Â  # CALCULATE AVERAGE MONTHS HELD
Â  Â  cohort_dates_map = cohort_dates.to_frame()
Â  Â  avg_months_held_df = calculate_average_months_held(df_cohort, ANALYSIS_END_DATE, cohort_dates_map)
Â  Â Â 
Â  Â  # CALCULATE INITIAL MIXES
Â  Â  initial_mix_df = calculate_initial_mixes(df_cohort)
Â  Â Â 
Â  Â  # reasons_df REMOVAL: Only keep basic details for terminated policies for the 'Detailed List' table if needed,
Â  Â  # but rename it to be less confusing and remove reason-specific logic.
Â  Â  policy_detail_cols = ['policy_nbr', 'term_date', 'issue_date', 'plan_code', 'tobacco', 'gender']
Â  Â  terminated_detail_df = df_cohort[df_cohort['Lapsed'] == 1].copy()
Â  Â  terminated_detail_df = terminated_detail_df[['Cohort_Week_Index', 'Lapse_Week', 'Policy_Month', 'Termination_Reason'] + [col for col in policy_detail_cols if col in terminated_detail_df.columns and col not in ['policy_nbr', 'issue_date', 'term_date']]].copy() # Added 'Termination_Reason' for detail table
Â  Â Â 
Â  Â  # Return the modified tuple
Â  Â  # MODIFIED: reasons_df is replaced by terminated_detail_df
Â  Â  return retention_rate, total_lapsed_policies, cohort_sizes, cohort_dates_map, mix_df, terminated_detail_df, avg_months_held_df, initial_mix_df

# --- 3. NEW AGGREGATION AND VISUALIZATION FUNCTIONS (UNCHANGED) ---
def aggregate_average_months_held(df_avg: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:
Â  Â  """ Aggregates the persistency across multiple cohorts within a time frame. """
Â  Â Â 
Â  Â  df_filtered = df_avg[
Â  Â  Â  Â  (df_avg['Cohort_Date'] >= start_date) &Â 
Â  Â  Â  Â  (df_avg['Cohort_Date'] <= end_date)
Â  Â  ].copy()
Â  Â Â 
Â  Â  if df_filtered.empty:
Â  Â  Â  Â  return pd.DataFrame()

Â  Â  # The aggregation needs to be weighted by Policy Count (Policies)
Â  Â  df_filtered['Weighted_Months'] = df_filtered['Avg_Months_Held'] * df_filtered['Policies']
Â  Â Â 
Â  Â  # Only aggregate by segments (Underwriting Class, Gender, Tobacco)
Â  Â  summary_df = df_filtered[df_filtered['Segment'] != 'Cohort Overall'].groupby(
Â  Â  Â  Â  ['Segment', 'Category']
Â  Â  ).agg(
Â  Â  Â  Â  Total_Weighted_Months=('Weighted_Months', 'sum'),
Â  Â  Â  Â  Total_Policies=('Policies', 'sum')
Â  Â  ).reset_index()
Â  Â Â 
Â  Â  summary_df['Avg_Months_Held'] = summary_df['Total_Weighted_Months'] / summary_df['Total_Policies']
Â  Â  summary_df.rename(columns={'Total_Policies': 'Policies'}, inplace=True)
Â  Â Â 
Â  Â  return summary_df[['Segment', 'Category', 'Avg_Months_Held', 'Policies']]


def plot_segment_avg_months(df_avg: pd.DataFrame, segment: str, title: str, key: str):
Â  Â  """ Generates a bar chart for average months held by segment (Used for Summary Tab). """
Â  Â  segment_df = df_avg[df_avg['Segment'] == segment].sort_values('Avg_Months_Held', ascending=False)
Â  Â Â 
Â  Â  if segment_df.empty:
Â  Â  Â  Â  st.warning(f"No data to display for {segment} in the selected timeframe.")
Â  Â  Â  Â  return

Â  Â  fig = px.bar(
Â  Â  Â  Â  segment_df,
Â  Â  Â  Â  x='Avg_Months_Held',
Â  Â  Â  Â  y='Category',
Â  Â  Â  Â  orientation='h',
Â  Â  Â  Â  title=title,
Â  Â  Â  Â  text=segment_df['Avg_Months_Held'].round(2).astype(str) + ' mos',
Â  Â  Â  Â  labels={'Avg_Months_Held': 'Average Months Held', 'Category': segment},
Â  Â  Â  Â  color='Policies', # Use policies as color intensity
Â  Â  Â  Â  color_continuous_scale=px.colors.sequential.Plasma
Â  Â  )
Â  Â  fig.update_traces(textposition='outside')
Â  Â  fig.update_layout(yaxis={'categoryorder': 'total ascending'},
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xaxis_title="Average Months Held (Persistency)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  height=max(400, 30 * len(segment_df)))
Â  Â  st.plotly_chart(fig, use_container_width=True, key=key)


def plot_segment_avg_months_single_cohort(df_avg: pd.DataFrame, cohort_index: int, segment: str, cohort_map: dict):
Â  Â  """ Generates a bar chart for average months held for a single cohort, segmented by attribute. """
Â  Â Â 
Â  Â  df_cohort = df_avg[
Â  Â  Â  Â  (df_avg['Cohort_Index'] == cohort_index) &Â 
Â  Â  Â  Â  (df_avg['Segment'] == segment)
Â  Â  ].sort_values('Avg_Months_Held', ascending=False)
Â  Â Â 
Â  Â  if df_cohort.empty:
Â  Â  Â  Â  st.warning(f"No data to display for {segment} in Cohort {cohort_index}.")
Â  Â  Â  Â  return

Â  Â  # Extract overall average for the title
Â  Â  overall_avg = df_avg[
Â  Â  Â  Â  (df_avg['Cohort_Index'] == cohort_index) &Â 
Â  Â  Â  Â  (df_avg['Segment'] == 'Cohort Overall')
Â  Â  ]['Avg_Months_Held'].iloc[0] if not df_avg.empty else 0.0

Â  Â  st.subheader(f"Avg Months Held by {segment} in {cohort_map[cohort_index]}")
Â  Â  st.markdown(f"**Cohort Overall Average:** `{overall_avg:.2f} months` (Policies: {df_cohort['Policies'].sum():,})")

Â  Â  fig = px.bar(
Â  Â  Â  Â  df_cohort,
Â  Â  Â  Â  x='Avg_Months_Held',
Â  Â  Â  Â  y='Category',
Â  Â  Â  Â  orientation='h',
Â  Â  Â  Â  text=df_cohort['Avg_Months_Held'].round(2).astype(str) + ' mos',
Â  Â  Â  Â  labels={'Avg_Months_Held': 'Average Months Held', 'Category': segment},
Â  Â  Â  Â  color='Policies',
Â  Â  Â  Â  color_continuous_scale=px.colors.sequential.Plasma
Â  Â  )
Â  Â  fig.update_traces(textposition='outside')
Â  Â  fig.update_layout(yaxis={'categoryorder': 'total ascending'},
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xaxis_title="Average Months Held (Persistency)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  height=max(400, 30 * len(df_cohort)))
Â  Â  # Added unique key
Â  Â  st.plotly_chart(fig, use_container_width=True, key=f'persistency_bar_{cohort_index}_{segment.replace(" ", "_")}')

def plot_initial_mix_trends(df_mix: pd.DataFrame, segment: str, cohort_dates_df: pd.DataFrame):
Â  Â  """ Generates a stacked area chart showing initial mix trends over time. """
Â  Â Â 
Â  Â  df_segment = df_mix[df_mix['Segment'] == segment]
Â  Â Â 
Â  Â  if df_segment.empty:
Â  Â  Â  Â  st.warning(f"No initial mix data available for {segment}.")
Â  Â  Â  Â  return
Â  Â  Â  Â Â 
Â  Â  df_segment = pd.merge(df_segment, cohort_dates_df.reset_index(), on='Cohort_Week_Index')
Â  Â Â 
Â  Â  fig = px.area(
Â  Â  Â  Â  df_segment,
Â  Â  Â  Â  x='Cohort_Start_Date',
Â  Â  Â  Â  y='Proportion',
Â  Â  Â  Â  color='Category',
Â  Â  Â  Â  line_group='Category',
Â  Â  Â  Â  title=f'Initial Policy Mix by {segment} Over Time',
Â  Â  Â  Â  labels={'Cohort_Start_Date': 'Cohort Week Start Date', 'Proportion': 'Initial Policy Proportion (%)', 'Category': segment},
Â  Â  Â  Â  groupnorm='percent' # Stacks to 100%
Â  Â  )
Â  Â  fig.update_layout(
Â  Â  Â  Â  yaxis_range=[0, 100],
Â  Â  Â  Â  yaxis_title="Initial Policy Proportion (%)",
Â  Â  Â  Â  hovermode="x unified"
Â  Â  )
Â  Â  # Added unique key
Â  Â  st.plotly_chart(fig, use_container_width=True, key=f'mix_trend_{segment.replace(" ", "_")}')

# --- NEW FUNCTION FOR CONDITIONAL FORMATTING ---
def color_retention_matrix(df: pd.DataFrame):
Â  Â  """ Applies green-to-red conditional formatting to the retention matrix. """
Â  Â Â 
Â  Â  # Define the custom 10-shade colormap from Green (high retention) to Red (low retention)
Â  Â  # Note: Green is first (0-10%) and Red is last (90-100%) for retention rate
Â  Â  colors = ["#f65420", "#f1733e", "#eb915d", "#e6b07c", "#e0cf9b", "#c9e29f", "#b6d58e", "#a3c87e", "#8fbc6d", "#6aa84f"]
Â  Â  cmap = LinearSegmentedColormap.from_list("RedToGreen", colors, N=10)
Â  Â Â 
Â  Â  # Apply the background gradient
Â  Â  return df.style.background_gradient(
Â  Â  Â  Â  cmap=cmap,Â 
Â  Â  Â  Â  vmin=0,Â 
Â  Â  Â  Â  vmax=100,Â 
Â  Â  Â  Â  subset=pd.IndexSlice[:, df.columns[0]:] # Apply to all columns except index
Â  Â  # MODIFICATION: REMOVED 'null_color' ARGUMENT TO FIX CRITICAL ERROR
Â  Â  ).format("{:.2f}").highlight_null()

# --------------------------------------------------------------------------------------------------
# --- Streamlit App Layout ---
# --------------------------------------------------------------------------------------------------


st.set_page_config(layout="wide", page_title="Weekly Cohort Retention Analysis")


st.title("ðŸš€ Local Cohort Retention and Termination Analysis") # MODIFIED: Removed 'Reason'
st.markdown(f"Data is automatically loaded from the configured local directory: `{BASE_PATH}`")

# --- Initial Data Loading ---
try:
Â  Â  # MODIFICATION: Changed how initial_df and earliest_date are loaded
Â  Â  # Compute CSV mtime and pass it to loader so Streamlit cache invalidates when the file changes
Â  Â  csv_path_for_cache = os.path.join(os.path.dirname(__file__), 'randompolicydata.csv')
Â  Â  csv_mtime = os.path.getmtime(csv_path_for_cache) if os.path.exists(csv_path_for_cache) else None
Â  Â  with st.spinner("Scanning directory and merging files (This may take a moment)..."):
Â  Â  Â  Â  initial_df, earliest_date = load_and_merge_data(POLICY_FILE_PATTERN, NSF_FILE_PATTERN, CANCELLED_FILE_PATTERN, cache_buster=csv_mtime)

Â  Â  if initial_df.empty or pd.isna(earliest_date):
Â  Â  Â  Â  st.error(f"âŒ Could not load or merge data, or no valid 'app_recvd_date' found. Please check the `BASE_PATH` ({BASE_PATH}) and ensure files are present.")
Â  Â  Â  Â  st.stop()

Â  Â  st.success(f"âœ… Data loaded successfully. Total policies merged: {len(initial_df):,}")

Â  Â  # --- Sidebar for Data Filtering ---
Â  Â  st.sidebar.header("1. Analysis Configuration")
Â  Â Â 
Â  Â  # --- START MODIFICATION FOR MIN DATE CONSTRAINT ---
Â  Â  MIN_ALLOWED_DATE = pd.to_datetime('2024-01-01').date()
Â  Â Â 
Â  Â  latest_available_date = initial_df['app_recvd_date'].max()
Â  Â  data_earliest_date = earliest_date.date()
Â  Â Â 
Â  Â  # The default should be the earliest available date in the data, but no earlier than MIN_ALLOWED_DATE
Â  Â  default_cohort_start = max(data_earliest_date, MIN_ALLOWED_DATE)
Â  Â Â 
Â  Â  cohort_start_date_input = st.sidebar.date_input(
Â  Â  Â  Â  "Cohort Start Date (App Recvd Filter)",Â 
Â  Â  Â  Â  value=default_cohort_start,
Â  Â  Â  Â  min_value=MIN_ALLOWED_DATE, # Set minimum to Jan 1, 2024
Â  Â  Â  Â  max_value=latest_available_date.date(),
Â  Â  Â  Â  help=f"Select a cohort start date. Restricted to be no earlier than {MIN_ALLOWED_DATE.strftime('%Y-%m-%d')}."
Â  Â  )
Â  Â  cohort_start_date = pd.to_datetime(cohort_start_date_input)
Â  Â  # --- END MODIFICATION FOR MIN DATE CONSTRAINT ---

Â  Â  analysis_end_date_input = st.sidebar.date_input("Analysis Cut-off Date", value=ANALYSIS_END_DATE)
Â  Â  ANALYSIS_END_DATE = pd.to_datetime(analysis_end_date_input)

Â  Â  st.sidebar.header("2. Data Filtering")

Â  Â  def get_unique_values(df, col_name):
Â  Â  Â  Â  if col_name in df.columns:
Â  Â  Â  Â  Â  Â  # Added a robust way to get unique, non-null values
Â  Â  Â  Â  Â  Â  return df[col_name].astype(str).str.strip().replace('nan', pd.NA).dropna().unique().tolist()
Â  Â  Â  Â  return []

Â  Â  # Get ALL unique plan codes from the initially loaded data
Â  Â  all_plan_options = get_unique_values(initial_df, 'plan_code')
Â  Â  luminary_plan_codes = MAPPING_PLAN_CODES
Â  Â  other_plan_codes = [p for p in all_plan_options if p not in luminary_plan_codes]

Â  Â  # --- Plan Code Filter Group Selection ---
Â  Â  st.sidebar.subheader("Filter by Plan Code Group")
Â  Â  plan_group_selection = st.sidebar.radio(
Â  Â  Â  Â  "Plan Code Group:",
Â  Â  Â  Â  ('All', 'Luminary', 'Other', 'Custom Select'),
Â  Â  Â  Â  index=0, # Default to 'All'
Â  Â  Â  Â  key='plan_code_group_select'
Â  Â  )

Â  Â  selected_plans_for_analysis = []
Â  Â Â 
Â  Â  if plan_group_selection == 'All':
Â  Â  Â  Â  selected_plans_for_analysis = all_plan_options
Â  Â  Â  Â  st.sidebar.info(f"Filtering for **{len(selected_plans_for_analysis)}** total unique plan codes.")
Â  Â  elif plan_group_selection == 'Luminary':
Â  Â  Â  Â  selected_plans_for_analysis = luminary_plan_codes
Â  Â  Â  Â  st.sidebar.info(f"Filtering for **{len(selected_plans_for_analysis)}** Luminary plan codes.")
Â  Â  elif plan_group_selection == 'Other':
Â  Â  Â  Â  selected_plans_for_analysis = other_plan_codes
Â  Â  Â  Â  st.sidebar.info(f"Filtering for **{len(other_plan_codes)}** 'Other' plan codes.")
Â  Â  elif plan_group_selection == 'Custom Select':
Â  Â  Â  Â  st.sidebar.markdown("---")
Â  Â  Â  Â  st.sidebar.subheader("Custom Plan Code Selection")
Â  Â  Â  Â  # Multiselect for ALL plan codes when 'Custom Select' is chosen
Â  Â  Â  Â  selected_plans_for_analysis = st.sidebar.multiselect(
Â  Â  Â  Â  Â  Â  "Select specific Plan Codes:",Â 
Â  Â  Â  Â  Â  Â  options=all_plan_options,Â 
Â  Â  Â  Â  Â  Â  default=all_plan_options,
Â  Â  Â  Â  Â  Â  key='plan_code_multiselect_custom'
Â  Â  Â  Â  )
Â  Â  Â  Â  st.sidebar.info(f"Custom selection: **{len(selected_plans_for_analysis)}** plan codes selected.")
Â  Â Â 
Â  Â  # Use the result of the group/custom selection as the filter argument
Â  Â  selected_plans = selected_plans_for_analysis
Â  Â  # --- END Plan Code Filter Group Selection ---

Â  Â  selected_tobacco = []
Â  Â  if 'tobacco' in initial_df.columns:
Â  Â  Â  Â  tobacco_options = get_unique_values(initial_df, 'tobacco')
Â  Â  Â  Â  selected_tobacco = st.sidebar.multiselect("Filter by Tobacco Status", options=tobacco_options, default=tobacco_options)

Â  Â  selected_gender = []
Â  Â  if 'gender' in initial_df.columns:
Â  Â  Â  Â  gender_options = get_unique_values(initial_df, 'gender')
Â  Â  Â  Â  selected_gender = st.sidebar.multiselect("Filter by Gender", options=gender_options, default=gender_options)
Â  Â Â 
Â  Â  # --- Case Status Filter ---
Â  Â  case_status_options = ['Active', 'Inactive']
Â  Â  selected_case_status = st.sidebar.multiselect(
Â  Â  Â  Â  "Filter by Case Status",Â 
Â  Â  Â  Â  options=case_status_options,Â 
Â  Â  Â  Â  default=case_status_options,Â 
Â  Â  Â  Â  help="Active: Not lapsed as of Analysis Cut-off Date. Inactive: Lapsed as of Analysis Cut-off Date."
Â  Â  )
Â  Â  # --------------------------

Â  Â  # --- Run Cohort Analysis ---
Â  Â  with st.spinner("Performing Cohort Analysis..."):
Â  Â  Â  Â  # MODIFIED: removed reasons_df from the tuple
Â  Â  Â  Â  retention_matrix, total_lapsed_policies, cohort_sizes, cohort_dates_df, mix_df, terminated_detail_df, avg_months_held_df, initial_mix_df = perform_cohort_analysis(
Â  Â  Â  Â  Â  Â  initial_df.copy(),Â 
Â  Â  Â  Â  Â  Â  cohort_start_date,
Â  Â  Â  Â  Â  Â  selected_plans, # Pass the resolved list of plan codes
Â  Â  Â  Â  Â  Â  selected_tobacco,
Â  Â  Â  Â  Â  Â  selected_gender,
Â  Â  Â  Â  Â  Â  selected_case_status
Â  Â  Â  Â  )

Â  Â  if retention_matrix.empty:
Â  Â  Â  Â  st.warning("No policies found matching the selected filters and cohort start date.")
Â  Â  Â  Â  st.stop()
Â  Â Â 
Â  Â  st.header("Results Summary")
Â  Â  col1, col2 = st.columns(2)
Â  Â  with col1:
Â  Â  Â  Â  st.metric("Total Cohort Policies", value=f"{cohort_sizes.sum():,}")
Â  Â  with col2:
Â  Â  Â  Â  st.metric("Total Policies Lapsed", value=f"{total_lapsed_policies:,}")

Â  Â  cohort_map = {index: f"Cohort {index} (Week of {date.strftime('%Y-%m-%d')})" for index, date in cohort_dates_df['Cohort_Start_Date'].items()}


Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- MAIN CONTENT TABS ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # MODIFIED: Removed 'Cross-Cohort Trends' and 'Cohort Comparison' tabs
Â  Â  cohort_tab, persistency_summary_tab, persistency_deep_dive_tab, historical_mixes_tab = st.tabs([
Â  Â  Â  Â  "ðŸ“Š Cohort Deep Dive",Â 
Â  Â  Â  Â  "ðŸ’° Cohort Persistency Summary",Â 
Â  Â  Â  Â  "ðŸ”Ž Single Cohort Persistency Deep Dive",Â 
Â  Â  Â  Â  "ðŸ“ˆ Historical Mixes"
Â  Â  ])

Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- TAB 1: Selected Cohort Deep Dive (Retention and Mix) ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  with cohort_tab:
Â  Â  Â  Â  st.header("Cohort Retention Matrix (%)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # MODIFICATION: Apply conditional formatting
Â  Â  Â  Â  st.dataframe(color_retention_matrix(retention_matrix), use_container_width=True, key='retention_matrix_df')
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Retention Plot
Â  Â  Â  Â  plot_df = retention_matrix.reset_index().melt(id_vars='Cohort_Week_Index', var_name='Tenure_Week', value_name='Retention_Rate')
Â  Â  Â  Â  plot_df['Tenure_Week_Num'] = plot_df['Tenure_Week'].str.split('_').str[1].astype(int)

Â  Â  Â  Â  st.header("Retention Rate Over Tenure (All Cohorts)")
Â  Â  Â  Â  fig = px.line(plot_df, x='Tenure_Week_Num', y='Retention_Rate', color='Cohort_Week_Index', title='Retention Rate by Cohort', markers=True, color_discrete_sequence=px.colors.qualitative.Bold)
Â  Â  Â  Â  fig.update_layout(xaxis_title="Tenure Week", yaxis_title="Retention Rate (%)", yaxis_range=[0, 100])
Â  Â  Â  Â  st.plotly_chart(fig, use_container_width=True, key='retention_rate_plot_all')
Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  # Cohort Deep Dive & Mix Analysis
Â  Â  Â  Â  st.header("Single Cohort Deep Dive: Retention Curve & Mix")
Â  Â  Â  Â  selected_cohort_index = list(cohort_map.keys())[0] # Default selection
Â  Â  Â  Â Â 
Â  Â  Â  Â  col_sel, col_group = st.columns([0.6, 0.4])
Â  Â  Â  Â  with col_sel:
Â  Â  Â  Â  Â  Â  selected_cohort_index = st.selectbox("Select a Cohort:", options=list(cohort_map.keys()), format_func=lambda x: cohort_map[x], key='deep_dive_cohort')
Â  Â  Â  Â  with col_group:
Â  Â  Â  Â  Â  Â  group_by_col = st.radio("Group Mix Chart By:", ('Underwriting Class', 'Gender', 'Tobacco'), key='deep_dive_group')
Â  Â  Â  Â  group_col_name = {'Underwriting Class': 'underwriting_class', 'Gender': 'gender', 'Tobacco': 'tobacco'}[group_by_col]
Â  Â  Â  Â Â 
Â  Â  Â  Â  single_mix_df = mix_df[mix_df['Cohort_Week_Index'] == selected_cohort_index].copy()
Â  Â  Â  Â  max_tenure_cohort = int(single_mix_df['Tenure_Weeks'].max()) if not single_mix_df.empty else 0
Â  Â  Â  Â  mix_data = []
Â  Â  Â  Â  single_retention_df = plot_df[plot_df['Cohort_Week_Index'] == selected_cohort_index].dropna(subset=['Retention_Rate'])

Â  Â  Â  Â  for week_num in range(1, max_tenure_cohort + 1):
Â  Â  Â  Â  Â  Â  policies_active_at_end_of_week = single_mix_df[(single_mix_df['Lapsed'] == 0) | (single_mix_df['Lapse_Week'] > week_num)]
Â  Â  Â  Â  Â  Â  if policies_active_at_end_of_week.empty: break
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # FIX: Only attempt to group by the column if it exists in the filtered DataFrame
Â  Â  Â  Â  Â  Â  if group_col_name in policies_active_at_end_of_week.columns:
Â  Â  Â  Â  Â  Â  Â  Â  group_counts = policies_active_at_end_of_week.groupby(group_col_name)['Lapsed'].count().reset_index(name='Active_Policies')
Â  Â  Â  Â  Â  Â  Â  Â  total_active = group_counts['Active_Policies'].sum()
Â  Â  Â  Â  Â  Â  Â  Â  if total_active > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  group_counts['Proportion_Active'] = (group_counts['Active_Policies'] / total_active) * 100
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  group_counts['Tenure_Week_Num'] = week_num
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mix_data.append(group_counts)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # If the column is missing after filtering (e.g., all policies had same value), skip the mix calculation for this segment
Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â Â 
Â  Â  Â  Â  if mix_data and not single_retention_df.empty:
Â  Â  Â  Â  Â  Â  final_mix_df = pd.concat(mix_data)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Plot 1: Retention Curve & Mix (Dual Axis)
Â  Â  Â  Â  Â  Â  fig_combined = make_subplots(specs=[[{"secondary_y": True}]])
Â  Â  Â  Â  Â  Â  for group in final_mix_df[group_col_name].unique():
Â  Â  Â  Â  Â  Â  Â  Â  group_data = final_mix_df[final_mix_df[group_col_name] == group]
Â  Â  Â  Â  Â  Â  Â  Â  fig_combined.add_trace(go.Scatter(x=group_data['Tenure_Week_Num'], y=group_data['Proportion_Active'], name=f"Mix: {group}", mode='lines', fill='tonexty', line=dict(width=0), stackgroup='one', hovertemplate=f"Week: %{{x}}<br>{group_by_col}: {group}<br>Mix: %{{y:.1f}}%<extra></extra>"), secondary_y=False)
Â  Â  Â  Â  Â  Â  fig_combined.add_trace(go.Scatter(x=single_retention_df['Tenure_Week_Num'], y=single_retention_df['Retention_Rate'], name='Overall Retention Rate', mode='lines+markers', line=dict(color='black', width=3), marker=dict(symbol='circle', size=6), hovertemplate="Week: %{x}<br>Retention: %{y:.2f}%<extra></extra>"), secondary_y=True)
Â  Â  Â  Â  Â  Â  fig_combined.update_layout(title_text=f"Retention Curve (Line) & Policy Mix (Area) for {cohort_map[selected_cohort_index]}", height=500, hovermode="x unified", legend_title_text="Legend")
Â  Â  Â  Â  Â  Â  fig_combined.update_xaxes(title_text="Tenure Week")
Â  Â  Â  Â  Â  Â  fig_combined.update_yaxes(title_text="Active Policy Mix (%)", secondary_y=False, range=[0, 100], showgrid=False)
Â  Â  Â  Â  Â  Â  fig_combined.update_yaxes(title_text="Overall Retention Rate (%)", secondary_y=True, range=[0, 100], showgrid=True)
Â  Â  Â  Â  Â  Â  st.plotly_chart(fig_combined, use_container_width=True, key='retention_curve_mix_plot')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.subheader(f"Absolute Count of Active Policies by {group_by_col}")
Â  Â  Â  Â  Â  Â  fig_bars = px.bar(final_mix_df, x='Tenure_Week_Num', y='Active_Policies', color=group_col_name, barmode='group', title=f'Active Policy Counts for {cohort_map[selected_cohort_index]}')
Â  Â  Â  Â  Â  Â  fig_bars.update_layout(xaxis_title="Tenure Week", yaxis_title="Count of Active Policies", legend_title_text=group_col_name, height=400)
Â  Â  Â  Â  Â  Â  st.plotly_chart(fig_bars, use_container_width=True, key='active_policy_count_plot')
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # LAPSE REASON ANALYSIS REMOVED
Â  Â  Â  Â  Â  Â  # st.header("Lapse Reason Breakdown Over Tenure (Single Cohort)") ... (Removed all charts related to reason analysis)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # TERMINATED POLICY DETAIL TABLE (Simplified to remove heavy 'reasons' focus)
Â  Â  Â  Â  Â  Â  st.header("ðŸ“ Detailed List of Terminated Policies (Reference Data)")
Â  Â  Â  Â  Â  Â  # MODIFIED: Use terminated_detail_df instead of reasons_df
Â  Â  Â  Â  Â  Â  detail_table_df = terminated_detail_df[terminated_detail_df['Cohort_Week_Index'] == selected_cohort_index].copy()

Â  Â  Â  Â  Â  Â  # Fetch the policies again from the main cohort DF to get all columns (issue/term dates, policy_nbr)
Â  Â  Â  Â  Â  Â  detail_policy_nrs = detail_table_df['policy_nbr'].unique()
Â  Â  Â  Â  Â  Â  # Re-filter the main df_cohort which has all policy details including dates
Â  Â  Â  Â  Â  Â  full_detail_df = initial_df[initial_df['policy_nbr'].isin(detail_policy_nrs)].copy()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Merge the simplified terminated_detail_df back to get Cohort/Lapse Week/Reason
Â  Â  Â  Â  Â  Â  merge_cols = ['policy_nbr', 'term_date', 'issue_date', 'plan_code', 'tobacco', 'gender', 'Termination_Reason', 'Lapse_Week', 'Policy_Month']
Â  Â  Â  Â  Â  Â  detail_table_df = full_detail_df[merge_cols].dropna(subset=['term_date']).copy()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # FIX: Check if date columns exist and are datetime before formatting
Â  Â  Â  Â  Â  Â  if 'term_date' in detail_table_df.columns and pd.api.types.is_datetime64_any_dtype(detail_table_df['term_date']):
Â  Â  Â  Â  Â  Â  Â  Â  detail_table_df['Terminated_Date'] = detail_table_df['term_date'].dt.strftime('%Y-%m-%d').fillna('N/A')
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  detail_table_df['Terminated_Date'] = 'N/A'
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if 'issue_date' in detail_table_df.columns and pd.api.types.is_datetime64_any_dtype(detail_table_df['issue_date']):
Â  Â  Â  Â  Â  Â  Â  Â  detail_table_df['Issue_Date'] = detail_table_df['issue_date'].dt.strftime('%Y-%m-%d').fillna('N/A')
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  detail_table_df['Issue_Date'] = 'N/A'
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # MODIFIED: Retained 'Termination_Reason' for the table as it's a core attribute of the terminated policy, even without a chart.
Â  Â  Â  Â  Â  Â  display_cols = ['policy_nbr', 'Terminated_Date', 'Issue_Date', 'Lapse_Week',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'Termination_Reason', 'plan_code', 'gender', 'tobacco']
Â  Â  Â  Â  Â  Â  final_display_cols = [col for col in display_cols if col in detail_table_df.columns]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.dataframe(detail_table_df[final_display_cols].set_index('policy_nbr'), use_container_width=True, key='terminated_policies_detail')
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- REFERENCE DATA FOR SINGLE COHORT ---
Â  Â  Â  Â  Â  Â  st.subheader("Reference Data: Active Policy Mix Over Tenure")
Â  Â  Â  Â  Â  Â  final_mix_df_display = final_mix_df.copy()
Â  Â  Â  Â  Â  Â  final_mix_df_display.rename(columns={'Tenure_Week_Num': 'Tenure Week', group_col_name: 'Category', 'Active_Policies': 'Active Policy Count', 'Proportion_Active': 'Mix (%)'}, inplace=True)
Â  Â  Â  Â  Â  Â  st.dataframe(final_mix_df_display[['Tenure Week', 'Category', 'Active Policy Count', 'Mix (%)']].style.format({'Mix (%)': '{:.2f}'}), use_container_width=True, key='ref_mix_data_deep_dive')

Â  Â  Â  Â  Â  Â  # REMOVED: Reference Data: Lapse Reason Count and Proportion
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.info("No active policies found for the selected cohort or filters to generate the mix chart.")
Â  Â  Â  Â  Â  Â Â 

Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- TAB 2: Cross-Cohort Trends (REMOVED: Lapse Reason Trends) ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # with cross_cohort_tab: (ENTIRE TAB REMOVED)


Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- TAB 3: Cohort Comparison (REMOVED: Lapse Reason Comparison) ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # with cohort_comparison_tab: (ENTIRE TAB REMOVED)


Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- TAB 4: Cohort Persistency Summary (Now the 2nd visible tab) ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  with persistency_summary_tab:
Â  Â  Â  Â  st.header("ðŸ’° Cohort Persistency Summary (Aggregated Timeframe)")
Â  Â  Â  Â  st.info("This view aggregates the Average Months Held for all cohorts that started within the selected time period, segmented by policy attributes.")

Â  Â  Â  Â  if avg_months_held_df.empty:
Â  Â  Â  Â  Â  Â  st.warning("No data available to calculate average months held with current filters.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  min_date = cohort_dates_df['Cohort_Start_Date'].min()
Â  Â  Â  Â  Â  Â  max_date = cohort_dates_df['Cohort_Start_Date'].max()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  col_date1, col_date2 = st.columns(2)

Â  Â  Â  Â  Â  Â  with col_date1:
Â  Â  Â  Â  Â  Â  Â  Â  start_date_filter = st.date_input("Filter Cohort Start Date (From)", value=min_date, min_value=min_date, max_value=max_date, key='persistency_summary_start')
Â  Â  Â  Â  Â  Â  with col_date2:
Â  Â  Â  Â  Â  Â  Â  Â  end_date_filter = st.date_input("Filter Cohort Start Date (To)", value=max_date, min_value=min_date, max_value=max_date, key='persistency_summary_end')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  summary_data = aggregate_average_months_held(avg_months_held_df, pd.to_datetime(start_date_filter), pd.to_datetime(end_date_filter))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if summary_data.empty:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("No cohorts were found within the selected date range.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Overall Average Calculation for the filtered group
Â  Â  Â  Â  Â  Â  Â  Â  overall_summary_df = avg_months_held_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (avg_months_held_df['Cohort_Date'] >= pd.to_datetime(start_date_filter)) &Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (avg_months_held_df['Cohort_Date'] <= pd.to_datetime(end_date_filter)) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (avg_months_held_df['Segment'] == 'Cohort Overall')
Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not overall_summary_df.empty and overall_summary_df['Policies'].sum() > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  overall_avg_months = (overall_summary_df['Avg_Months_Held'] * overall_summary_df['Policies']).sum() / overall_summary_df['Policies'].sum()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Overall Average Months Held (Filtered Cohorts)", f"{overall_avg_months:.2f} months", delta=None)
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Overall Average Months Held (Filtered Cohorts)", "N/A", delta=None)


Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  Â  Â  Â  Â  # Plotting the aggregated segments
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Persistency by Underwriting Class")
Â  Â  Â  Â  Â  Â  Â  Â  plot_segment_avg_months(summary_data, 'Underwriting Class', 'Aggregated Avg Months Held by Underwriting Class', 'persistency_uw_summary')

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Persistency by Gender")
Â  Â  Â  Â  Â  Â  Â  Â  plot_segment_avg_months(summary_data, 'Gender', 'Aggregated Avg Months Held by Gender', 'persistency_gender_summary')

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Persistency by Tobacco Status")
Â  Â  Â  Â  Â  Â  Â  Â  plot_segment_avg_months(summary_data, 'Tobacco', 'Aggregated Avg Months Held by Tobacco Status', 'persistency_tobacco_summary')

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # --- REFERENCE DATA FOR PERSISTENCY SUMMARY ---
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Reference Data: Aggregated Persistency Data Table")
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(summary_data.round({'Avg_Months_Held': 2}).rename(columns={'Policies': 'Policy Count', 'Avg_Months_Held': 'Avg Months Held'}), use_container_width=True, key='persistency_aggregated_table')


Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- TAB 5: Single Cohort Persistency Deep Dive (Now the 3rd visible tab) ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  with persistency_deep_dive_tab:
Â  Â  Â  Â  st.header("ðŸ”Ž Single Cohort Persistency Deep Dive")
Â  Â  Â  Â  st.info("This view isolates one selected cohort and analyzes the average months held for its sub-segments.")

Â  Â  Â  Â  if avg_months_held_df.empty:
Â  Â  Â  Â  Â  Â  st.warning("No data available to calculate average months held with current filters.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  col_sel, col_group = st.columns([0.4, 0.6])
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with col_sel:
Â  Â  Â  Â  Â  Â  Â  Â  selected_persistency_cohort_index = st.selectbox(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Select Cohort for Analysis:",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  options=list(cohort_map.keys()),Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  format_func=lambda x: cohort_map[x],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key='persistency_cohort_select_deep_dive'
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with col_group:
Â  Â  Â  Â  Â  Â  Â  Â  segment_for_persistency = st.radio(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Segment Persistency By:",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('Underwriting Class', 'Gender', 'Tobacco'),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key='persistency_group_radio_deep_dive',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  horizontal=True
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  plot_segment_avg_months_single_cohort(
Â  Â  Â  Â  Â  Â  Â  Â  avg_months_held_df,Â 
Â  Â  Â  Â  Â  Â  Â  Â  selected_persistency_cohort_index,Â 
Â  Â  Â  Â  Â  Â  Â  Â  segment_for_persistency,Â 
Â  Â  Â  Â  Â  Â  Â  Â  cohort_map
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- REFERENCE DATA FOR SINGLE COHORT PERSISTENCY ---
Â  Â  Â  Â  Â  Â  st.subheader("Reference Data: Segmented Persistency Metrics (Selected Cohort)")
Â  Â  Â  Â  Â  Â  single_cohort_persistency_data = avg_months_held_df[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (avg_months_held_df['Cohort_Index'] == selected_persistency_cohort_index) &Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â (avg_months_held_df['Segment'] != 'Cohort Overall')
Â  Â  Â  Â  Â  Â  ].copy()
Â  Â  Â  Â  Â  Â  single_cohort_persistency_data = single_cohort_persistency_data[['Segment', 'Category', 'Avg_Months_Held', 'Policies']].rename(
Â  Â  Â  Â  Â  Â  Â  Â  columns={'Policies': 'Policy Count', 'Avg_Months_Held': 'Avg Months Held'}
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  st.dataframe(single_cohort_persistency_data.style.format({"Avg Months Held": "{:.2f}"}), use_container_width=True, key='ref_persistency_single_cohort_data')

Â  Â  Â  Â  Â  Â  st.subheader("Reference Data: Overall Cohort Persistency Summary")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Create a table showing the overall cohort persistency
Â  Â  Â  Â  Â  Â  cohort_summary_table = avg_months_held_df[
Â  Â  Â  Â  Â  Â  Â  Â  avg_months_held_df['Segment'] == 'Cohort Overall'
Â  Â  Â  Â  Â  Â  ].copy().sort_values('Cohort_Index')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  cohort_summary_table['Cohort'] = 'Cohort ' + cohort_summary_table['Cohort_Index'].astype(str) + ' (' + cohort_summary_table['Cohort_Date'].dt.strftime('%Y-%m-%d') + ')'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  final_table = cohort_summary_table[['Cohort', 'Avg_Months_Held', 'Policies']].rename(
Â  Â  Â  Â  Â  Â  Â  Â  columns={'Avg_Months_Held': 'Avg Months Held', 'Policies': 'Policy Count'}
Â  Â  Â  Â  Â  Â  ).set_index('Cohort')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.dataframe(final_table.style.format({"Avg Months Held": "{:.2f}"}), use_container_width=True, key='persistency_summary_table_all')

Â  Â  # ----------------------------------------------------------------------------------
Â  Â  # --- TAB 6: Historical Mixes (Now the 4th visible tab) ---
Â  Â  # ----------------------------------------------------------------------------------
Â  Â  with historical_mixes_tab:
Â  Â  Â  Â  st.header("ðŸ“ˆ Initial Policy Mix Trends by Cohort")
Â  Â  Â  Â  st.info("This view shows how the demographic and underwriting mix of your new business has changed over time for the policies included in the analysis.")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if initial_mix_df.empty:
Â  Â  Â  Â  Â  Â  st.warning("No initial mix data available.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.subheader("1. Underwriting Class Mix Trend")
Â  Â  Â  Â  Â  Â  plot_initial_mix_trends(initial_mix_df, 'Underwriting Class', cohort_dates_df)

Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.subheader("2. Gender Mix Trend")
Â  Â  Â  Â  Â  Â  plot_initial_mix_trends(initial_mix_df, 'Gender', cohort_dates_df)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  Â  Â  st.subheader("3. Tobacco Status Mix Trend")
Â  Â  Â  Â  Â  Â  plot_initial_mix_trends(initial_mix_df, 'Tobacco', cohort_dates_df)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- REFERENCE DATA FOR HISTORICAL MIXES ---
Â  Â  Â  Â  Â  Â  st.subheader("Reference Data: Initial Mix Data Table")
Â  Â  Â  Â  Â  Â  mix_table = pd.merge(initial_mix_df, cohort_dates_df.reset_index(), on='Cohort_Week_Index')
Â  Â  Â  Â  Â  Â  mix_table['Cohort Date'] = mix_table['Cohort_Start_Date'].dt.strftime('%Y-%m-%d')
Â  Â  Â  Â  Â  Â  mix_table = mix_table[['Cohort Date', 'Cohort_Week_Index', 'Segment', 'Category', 'Policy_Count', 'Proportion']]
Â  Â  Â  Â  Â  Â  mix_table.rename(columns={'Cohort_Week_Index': 'Cohort Index', 'Policy_Count': 'Count', 'Proportion': 'Proportion (%)'}, inplace=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.dataframe(mix_table.round({'Proportion (%)': 2}), use_container_width=True, hide_index=True, key='initial_mix_data_table')


except Exception as e:
Â  Â  # Ensure the error is captured and displayed for debugging
Â  Â  st.error(f"A critical error occurred during file processing or analysis: {e}")
Â  Â  st.info("Double-check that the `BASE_PATH` is correct and all required files (`Policies_DBGA_*.csv`, `NSF_Activity_DBGA_*.csv`, `Cancelled_*.csv`) are present and properly formatted.")
